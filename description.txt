Raport z projektu
06-DUMALI0 Semestr zimowy 2024/2025
Cel projektu
Celem projektu było stworzenie modelu, który przewiduje ceny domów na podstawie danych dotyczących cech nieruchomości i jej otoczenia. Projekt oparto na danych z konkursu House Prices - Advanced Regression Techniques na platformie Kaggle. Modele zostały ocenione pod kątem zdolności do przewidywania zmiennej docelowej SalePrice.
Dane
Dane pochodzą z konkursu House Prices - Advanced Regression Techniques (link do konkursu).
•	Zbiór uczący zawierał 1460 przykładów i 79 cech opisujących domy, takie jak:
o	Numeryczne: GrLivArea, LotArea, OverallQual.
o	Kategoryczne: Neighborhood, HouseStyle, Exterior1st.
•	Zbiór testowy liczył 1459 przykładów (bez etykiety SalePrice).
•	Dane zawierały:
o	Braki danych: Niektóre cechy miały brakujące wartości (np. LotFrontage, GarageYrBlt).
o	Outliery: Obserwacje odstające w cechach takich jak GrLivArea.
Wstępne przetwarzanie danych:
1.	Braki danych:
o	Numeryczne wartości uzupełniono medianą.
o	Kategoryczne wartości uzupełniono trybem.
2.	Transformacje:
o	Log-transformacja zmiennych z prawoskośnym rozkładem (SalePrice, GrLivArea).
o	Skalowanie cech numerycznych do zakresu [0, 1].
3.	Kodowanie zmiennych kategorycznych:
o	One-Hot Encoding dla cech takich jak Neighborhood.

Modele
W projekcie porównano działanie 4 modeli:
1.	Regresja liniowa bez regularyzacji:
o	Cel: Punkt odniesienia dla bardziej zaawansowanych modeli.
o	Implementacja: W scikit-learn z LinearRegression.
2.	Regresja liniowa z regularyzacją (L2):
o	Cel: Zapobieganie nadmiernemu dopasowaniu przy wielu cechach.
o	Implementacja: W scikit-learn z Ridge.
o	Parametry: Optymalizacja parametru alpha metodą Grid Search.
3.	Gradient Boosting (XGBoost):
o	Cel: Wykorzystanie modeli drzew decyzyjnych do zwiększenia dokładności.
o	Implementacja: W XGBoost.
o	Parametry: Liczba estymatorów = 100, learning rate = 0.1.
4.	Sieć neuronowa (MLP):
o	Architektura: 2 warstwy ukryte (128 i 64 neurony) z funkcją aktywacji ReLU.
o	Optymalizator: Adam.
o	Implementacja: W TensorFlow/Keras.
o	Regularizacja: Dropout na poziomie 0.2 w warstwach ukrytych.






Ewaluacja
Do ewaluacji wykorzystano metryki regresji:
•	Root Mean Squared Error (RMSE).
•	Mean Absolute Error (MAE).
Model	RMSE	MAE
Regresja liniowa bez regularyzacji		
Regresja liniowa z regularyzacją		
Gradient Boosting (XGBoost)		
Sieć neuronowa (MLP)		
Wnioski
